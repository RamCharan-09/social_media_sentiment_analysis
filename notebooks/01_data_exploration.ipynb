{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Social Media Sentiment Analysis - 45K Training\n",
    "## Working with Real Sentiment140 Dataset (1.6M ‚Üí 45K)\n",
    "\n",
    "This notebook will:\n",
    "1. Load 45,000 tweets from the full 1.6M dataset\n",
    "2. Clean and process the data\n",
    "3. Prepare for machine learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.data_collector import DataCollector\n",
    "from src.data.data_cleaner import DataCleaner\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"üöÄ Social Media Sentiment Analysis - 45K Training\")\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(\"üìä Ready to process real Twitter data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check dataset and get info\n",
    "collector = DataCollector()\n",
    "\n",
    "print(\"üìÅ Checking for manually downloaded dataset...\")\n",
    "dataset_info = collector.get_dataset_info()\n",
    "\n",
    "if dataset_info:\n",
    "    print(\"\\n‚úÖ Dataset ready for processing!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Please download and place the dataset first\")\n",
    "    print(\"üì• Download from: https://www.kaggle.com/datasets/kazanova/sentiment140\")\n",
    "    print(\"üìÅ Place at: data/raw/sentiment140.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load 45,000 balanced samples\n",
    "print(\"üìä Loading 45,000 tweets from 1.6M dataset...\")\n",
    "print(\"‚è≥ This may take a moment to read the large file...\")\n",
    "\n",
    "# Load our target sample size\n",
    "data = collector.load_data(sample_size=45000)\n",
    "\n",
    "\n",
    "# data = collector.load_data(sample_size=1600000)\n",
    "\n",
    "if data is not None:\n",
    "    print(f\"\\nüéâ Success! Loaded {len(data):,} tweets\")\n",
    "    \n",
    "    # Show basic info\n",
    "    print(f\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"üìè Shape: {data.shape}\")\n",
    "    print(f\"üìã Columns: {list(data.columns)}\")\n",
    "    print(f\"üíæ Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.1f}MB\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nüîç Data Quality Check:\")\n",
    "    missing = data.isnull().sum()\n",
    "    for col, miss_count in missing.items():\n",
    "        if miss_count > 0:\n",
    "            print(f\"   ‚ö†Ô∏è {col}: {miss_count} missing values\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ {col}: No missing values\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load data. Check your dataset file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Explore sample tweets\n",
    "print(\"Sample tweets from our 45K dataset:\\n\")\n",
    "\n",
    "# Show a mix of positive and negative tweets\n",
    "negative_tweets = data[data['sentiment'] == 0].head(3)\n",
    "positive_tweets = data[data['sentiment'] == 4].head(3)\n",
    "\n",
    "print(\"üò¢ NEGATIVE TWEETS:\")\n",
    "for i, (_, tweet) in enumerate(negative_tweets.iterrows(), 1):\n",
    "    print(f\"{i}. {tweet['text'][:120]}...\" if len(tweet['text']) > 120 else f\"{i}. {tweet['text']}\")\n",
    "    print()\n",
    "\n",
    "print(\"üòä POSITIVE TWEETS:\")\n",
    "for i, (_, tweet) in enumerate(positive_tweets.iterrows(), 1):\n",
    "    print(f\"{i}. {tweet['text'][:120]}...\" if len(tweet['text']) > 120 else f\"{i}. {tweet['text']}\")\n",
    "    print()\n",
    "\n",
    "# Analyze text lengths\n",
    "text_lengths = data['text'].str.len()\n",
    "print(f\"üìè Text Length Statistics:\")\n",
    "print(f\"   Average: {text_lengths.mean():.1f} characters\")\n",
    "print(f\"   Shortest: {text_lengths.min()} characters\")\n",
    "print(f\"   Longest: {text_lengths.max()} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Clean the 45K tweets\n",
    "print(\"üßπ Starting cleaning process for 45,000 tweets...\")\n",
    "print(\"‚è≥ This will take about 2-3 minutes with progress tracking\")\n",
    "\n",
    "cleaner = DataCleaner()\n",
    "cleaned_data = cleaner.clean_dataset(data)\n",
    "\n",
    "if len(cleaned_data) > 0:\n",
    "    print(f\"\\nüéâ Cleaning Success!\")\n",
    "    \n",
    "    # Compare before and after\n",
    "    print(f\"\\nüìä Cleaning Results:\")\n",
    "    print(f\"   Original tweets: {len(data):,}\")\n",
    "    print(f\"   After cleaning: {len(cleaned_data):,}\")\n",
    "    print(f\"   Removal rate: {((len(data) - len(cleaned_data)) / len(data) * 100):.1f}%\")\n",
    "    \n",
    "    # Text length comparison\n",
    "    original_avg = data['text'].str.len().mean()\n",
    "    cleaned_avg = cleaned_data['cleaned_text'].str.len().mean()\n",
    "    print(f\"\\nüìè Text Length Reduction:\")\n",
    "    print(f\"   Original average: {original_avg:.1f} characters\")\n",
    "    print(f\"   Cleaned average: {cleaned_avg:.1f} characters\")\n",
    "    print(f\"   Reduction: {((original_avg - cleaned_avg) / original_avg * 100):.1f}%\")\n",
    "else:\n",
    "    print(\"‚ùå Cleaning failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Before/After examples\n",
    "print(\"üîç Before vs After Cleaning Examples:\\n\")\n",
    "\n",
    "# Show 5 examples of cleaning\n",
    "for i in range(5):\n",
    "    original = data.iloc[i]['text']\n",
    "    cleaned = cleaned_data.iloc[i]['cleaned_text']\n",
    "    sentiment = cleaned_data.iloc[i]['sentiment_label']\n",
    "    \n",
    "    print(f\"--- Example {i+1} [{sentiment.upper()}] ---\")\n",
    "    print(f\"BEFORE: {original}\")\n",
    "    print(f\"AFTER:  {cleaned}\")\n",
    "    print(f\"LENGTH: {len(original)} ‚Üí {len(cleaned)} characters\\n\")\n",
    "\n",
    "# Word cloud of most common words\n",
    "print(\"üìä Most Common Words After Cleaning:\")\n",
    "all_words = ' '.join(cleaned_data['cleaned_text']).split()\n",
    "word_freq = pd.Series(all_words).value_counts().head(20)\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Data visualization\n",
    "print(\"üìä Creating visualizations for our 45K dataset...\\n\")\n",
    "\n",
    "# Create comprehensive plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Sentiment distribution pie chart\n",
    "sentiment_counts = cleaned_data['sentiment_label'].value_counts()\n",
    "colors = ['#ff6b6b', '#4ecdc4']  # Red for negative, teal for positive\n",
    "ax1.pie(sentiment_counts.values, labels=['üò¢ Negative', 'üòä Positive'], \n",
    "        autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title(f'Sentiment Distribution\\n({len(cleaned_data):,} tweets)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Text length distribution\n",
    "text_lengths = cleaned_data['cleaned_text'].str.len()\n",
    "ax2.hist(text_lengths, bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Distribution of Tweet Lengths (After Cleaning)', fontweight='bold')\n",
    "ax2.set_xlabel('Number of Characters')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(text_lengths.mean(), color='red', linestyle='--', \n",
    "           label=f'Average: {text_lengths.mean():.1f}')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Top 15 words bar chart\n",
    "top_words = word_freq.head(15)\n",
    "ax3.barh(range(len(top_words)), top_words.values, color='lightcoral')\n",
    "ax3.set_yticks(range(len(top_words)))\n",
    "ax3.set_yticklabels(top_words.index)\n",
    "ax3.set_title('Top 15 Most Common Words', fontweight='bold')\n",
    "ax3.set_xlabel('Frequency')\n",
    "\n",
    "# 4. Processing statistics\n",
    "stats_data = {\n",
    "    'Original Dataset': len(data),\n",
    "    'After Cleaning': len(cleaned_data),\n",
    "    'Negative Tweets': len(cleaned_data[cleaned_data['sentiment_label'] == 'negative']),\n",
    "    'Positive Tweets': len(cleaned_data[cleaned_data['sentiment_label'] == 'positive'])\n",
    "}\n",
    "bars = ax4.bar(stats_data.keys(), stats_data.values(), \n",
    "               color=['lightblue', 'lightgreen', 'lightcoral', 'lightpink'])\n",
    "ax4.set_title('Dataset Processing Statistics', fontweight='bold')\n",
    "ax4.set_ylabel('Number of Tweets')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 500,\n",
    "             f'{int(height):,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save processed data\n",
    "print(\"üíæ Saving processed 45K dataset...\")\n",
    "\n",
    "# Save the cleaned data\n",
    "cleaner.save_data(cleaned_data)\n",
    "\n",
    "# Create a summary report\n",
    "summary = {\n",
    "    'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_tweets': len(data),\n",
    "    'cleaned_tweets': len(cleaned_data),\n",
    "    'removal_rate': f\"{((len(data) - len(cleaned_data)) / len(data) * 100):.1f}%\",\n",
    "    'negative_tweets': len(cleaned_data[cleaned_data['sentiment_label'] == 'negative']),\n",
    "    'positive_tweets': len(cleaned_data[cleaned_data['sentiment_label'] == 'positive']),\n",
    "    'avg_tweet_length': f\"{cleaned_data['cleaned_text'].str.len().mean():.1f}\",\n",
    "    'memory_usage_mb': f\"{cleaned_data.memory_usage(deep=True).sum() / 1024**2:.1f}\"\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv('../data/processed/processing_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\nüéâ Step 1 Complete!\")\n",
    "print(f\"\\nüìä Final Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Step 2: Feature Engineering!\")\n",
    "print(f\"\\n‚úÖ What we accomplished:\")\n",
    "print(f\"   üì• Loaded 45,000 tweets from 1.6M dataset\")\n",
    "print(f\"   üßπ Cleaned and processed all text\")\n",
    "print(f\"   ‚öñÔ∏è Maintained balanced positive/negative ratio\")\n",
    "print(f\"   üíæ Saved ready-to-use training data\")\n",
    "print(f\"   üìä Generated comprehensive analysis\")\n",
    "\n",
    "print(f\"\\nüéØ Next Step: Convert text to numerical features for ML training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
