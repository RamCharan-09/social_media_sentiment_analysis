{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Social Media Sentiment Analysis - 45K Training\n",
    "## Working with Real Sentiment140 Dataset (1.6M â†’ 45K)\n",
    "\n",
    "This notebook will:\n",
    "1. Load 45,000 tweets from the full 1.6M dataset\n",
    "2. Clean and process the data\n",
    "3. Prepare for machine learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.data_collector import DataCollector\n",
    "from src.data.data_cleaner import DataCleaner\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸš€ Social Media Sentiment Analysis - 45K Training\")\n",
    "print(\"âœ… All imports successful\")\n",
    "print(\"ğŸ“Š Ready to process real Twitter data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Check dataset and get info\n",
    "collector = DataCollector()\n",
    "\n",
    "print(\"ğŸ“ Checking for manually downloaded dataset...\")\n",
    "dataset_info = collector.get_dataset_info()\n",
    "\n",
    "if dataset_info:\n",
    "    print(\"\\nâœ… Dataset ready for processing!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Please download and place the dataset first\")\n",
    "    print(\"ğŸ“¥ Download from: https://www.kaggle.com/datasets/kazanova/sentiment140\")\n",
    "    print(\"ğŸ“ Place at: data/raw/sentiment140.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load 45,000 balanced samples\n",
    "print(\"ğŸ“Š Loading 45,000 tweets from 1.6M dataset...\")\n",
    "print(\"â³ This may take a moment to read the large file...\")\n",
    "\n",
    "# Load our target sample size\n",
    "data = collector.load_data(sample_size=45000)\n",
    "\n",
    "\n",
    "# data = collector.load_data(sample_size=1600000)\n",
    "\n",
    "if data is not None:\n",
    "    print(f\"\\nğŸ‰ Success! Loaded {len(data):,} tweets\")\n",
    "    \n",
    "    # Show basic info\n",
    "    print(f\"\\nğŸ“Š Dataset Overview:\")\n",
    "    print(f\"ğŸ“ Shape: {data.shape}\")\n",
    "    print(f\"ğŸ“‹ Columns: {list(data.columns)}\")\n",
    "    print(f\"ğŸ’¾ Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.1f}MB\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nğŸ” Data Quality Check:\")\n",
    "    missing = data.isnull().sum()\n",
    "    for col, miss_count in missing.items():\n",
    "        if miss_count > 0:\n",
    "            print(f\"   âš ï¸ {col}: {miss_count} missing values\")\n",
    "        else:\n",
    "            print(f\"   âœ… {col}: No missing values\")\n",
    "else:\n",
    "    print(\"âŒ Failed to load data. Check your dataset file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Explore sample tweets\n",
    "print(\"Sample tweets from our 45K dataset:\\n\")\n",
    "\n",
    "# Show a mix of positive and negative tweets\n",
    "negative_tweets = data[data['sentiment'] == 0].head(3)\n",
    "positive_tweets = data[data['sentiment'] == 4].head(3)\n",
    "\n",
    "print(\"ğŸ˜¢ NEGATIVE TWEETS:\")\n",
    "for i, (_, tweet) in enumerate(negative_tweets.iterrows(), 1):\n",
    "    print(f\"{i}. {tweet['text'][:120]}...\" if len(tweet['text']) > 120 else f\"{i}. {tweet['text']}\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ˜Š POSITIVE TWEETS:\")\n",
    "for i, (_, tweet) in enumerate(positive_tweets.iterrows(), 1):\n",
    "    print(f\"{i}. {tweet['text'][:120]}...\" if len(tweet['text']) > 120 else f\"{i}. {tweet['text']}\")\n",
    "    print()\n",
    "\n",
    "# Analyze text lengths\n",
    "text_lengths = data['text'].str.len()\n",
    "print(f\"ğŸ“ Text Length Statistics:\")\n",
    "print(f\"   Average: {text_lengths.mean():.1f} characters\")\n",
    "print(f\"   Shortest: {text_lengths.min()} characters\")\n",
    "print(f\"   Longest: {text_lengths.max()} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Clean the 45K tweets\n",
    "print(\"ğŸ§¹ Starting cleaning process for 45,000 tweets...\")\n",
    "print(\"â³ This will take about 2-3 minutes with progress tracking\")\n",
    "\n",
    "cleaner = DataCleaner()\n",
    "cleaned_data = cleaner.clean_dataset(data)\n",
    "\n",
    "if len(cleaned_data) > 0:\n",
    "    print(f\"\\nğŸ‰ Cleaning Success!\")\n",
    "    \n",
    "    # Compare before and after\n",
    "    print(f\"\\nğŸ“Š Cleaning Results:\")\n",
    "    print(f\"   Original tweets: {len(data):,}\")\n",
    "    print(f\"   After cleaning: {len(cleaned_data):,}\")\n",
    "    print(f\"   Removal rate: {((len(data) - len(cleaned_data)) / len(data) * 100):.1f}%\")\n",
    "    \n",
    "    # Text length comparison\n",
    "    original_avg = data['text'].str.len().mean()\n",
    "    cleaned_avg = cleaned_data['cleaned_text'].str.len().mean()\n",
    "    print(f\"\\nğŸ“ Text Length Reduction:\")\n",
    "    print(f\"   Original average: {original_avg:.1f} characters\")\n",
    "    print(f\"   Cleaned average: {cleaned_avg:.1f} characters\")\n",
    "    print(f\"   Reduction: {((original_avg - cleaned_avg) / original_avg * 100):.1f}%\")\n",
    "else:\n",
    "    print(\"âŒ Cleaning failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Before/After examples\n",
    "print(\"ğŸ” Before vs After Cleaning Examples:\\n\")\n",
    "\n",
    "# Show 5 examples of cleaning\n",
    "for i in range(5):\n",
    "    original = data.iloc[i]['text']\n",
    "    cleaned = cleaned_data.iloc[i]['cleaned_text']\n",
    "    sentiment = cleaned_data.iloc[i]['sentiment_label']\n",
    "    \n",
    "    print(f\"--- Example {i+1} [{sentiment.upper()}] ---\")\n",
    "    print(f\"BEFORE: {original}\")\n",
    "    print(f\"AFTER:  {cleaned}\")\n",
    "    print(f\"LENGTH: {len(original)} â†’ {len(cleaned)} characters\\n\")\n",
    "\n",
    "# Word cloud of most common words\n",
    "print(\"ğŸ“Š Most Common Words After Cleaning:\")\n",
    "all_words = ' '.join(cleaned_data['cleaned_text']).split()\n",
    "word_freq = pd.Series(all_words).value_counts().head(20)\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Data visualization\n",
    "print(\"ğŸ“Š Creating visualizations for our 45K dataset...\\n\")\n",
    "\n",
    "# Create comprehensive plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Sentiment distribution pie chart\n",
    "sentiment_counts = cleaned_data['sentiment_label'].value_counts()\n",
    "colors = ['#ff6b6b', '#4ecdc4']  # Red for negative, teal for positive\n",
    "ax1.pie(sentiment_counts.values, labels=['ğŸ˜¢ Negative', 'ğŸ˜Š Positive'], \n",
    "        autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title(f'Sentiment Distribution\\n({len(cleaned_data):,} tweets)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Text length distribution\n",
    "text_lengths = cleaned_data['cleaned_text'].str.len()\n",
    "ax2.hist(text_lengths, bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Distribution of Tweet Lengths (After Cleaning)', fontweight='bold')\n",
    "ax2.set_xlabel('Number of Characters')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(text_lengths.mean(), color='red', linestyle='--', \n",
    "           label=f'Average: {text_lengths.mean():.1f}')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Top 15 words bar chart\n",
    "top_words = word_freq.head(15)\n",
    "ax3.barh(range(len(top_words)), top_words.values, color='lightcoral')\n",
    "ax3.set_yticks(range(len(top_words)))\n",
    "ax3.set_yticklabels(top_words.index)\n",
    "ax3.set_title('Top 15 Most Common Words', fontweight='bold')\n",
    "ax3.set_xlabel('Frequency')\n",
    "\n",
    "# 4. Processing statistics\n",
    "stats_data = {\n",
    "    'Original Dataset': len(data),\n",
    "    'After Cleaning': len(cleaned_data),\n",
    "    'Negative Tweets': len(cleaned_data[cleaned_data['sentiment_label'] == 'negative']),\n",
    "    'Positive Tweets': len(cleaned_data[cleaned_data['sentiment_label'] == 'positive'])\n",
    "}\n",
    "bars = ax4.bar(stats_data.keys(), stats_data.values(), \n",
    "               color=['lightblue', 'lightgreen', 'lightcoral', 'lightpink'])\n",
    "ax4.set_title('Dataset Processing Statistics', fontweight='bold')\n",
    "ax4.set_ylabel('Number of Tweets')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 500,\n",
    "             f'{int(height):,}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save processed data\n",
    "print(\"ğŸ’¾ Saving processed 45K dataset...\")\n",
    "\n",
    "# Save the cleaned data\n",
    "cleaner.save_data(cleaned_data)\n",
    "\n",
    "# Create a summary report\n",
    "summary = {\n",
    "    'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_tweets': len(data),\n",
    "    'cleaned_tweets': len(cleaned_data),\n",
    "    'removal_rate': f\"{((len(data) - len(cleaned_data)) / len(data) * 100):.1f}%\",\n",
    "    'negative_tweets': len(cleaned_data[cleaned_data['sentiment_label'] == 'negative']),\n",
    "    'positive_tweets': len(cleaned_data[cleaned_data['sentiment_label'] == 'positive']),\n",
    "    'avg_tweet_length': f\"{cleaned_data['cleaned_text'].str.len().mean():.1f}\",\n",
    "    'memory_usage_mb': f\"{cleaned_data.memory_usage(deep=True).sum() / 1024**2:.1f}\"\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv('../data/processed/processing_summary.csv', index=False)\n",
    "\n",
    "print(f\"\\nğŸ‰ Step 1 Complete!\")\n",
    "print(f\"\\nğŸ“Š Final Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"   {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready for Step 2: Feature Engineering!\")\n",
    "print(f\"\\nâœ… What we accomplished:\")\n",
    "print(f\"   ğŸ“¥ Loaded 45,000 tweets from 1.6M dataset\")\n",
    "print(f\"   ğŸ§¹ Cleaned and processed all text\")\n",
    "print(f\"   âš–ï¸ Maintained balanced positive/negative ratio\")\n",
    "print(f\"   ğŸ’¾ Saved ready-to-use training data\")\n",
    "print(f\"   ğŸ“Š Generated comprehensive analysis\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Next Step: Convert text to numerical features for ML training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
